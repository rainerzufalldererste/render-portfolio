<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <title>Portfolio</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="assets/style.css">
</head>

<body>
  <div class="title"></div>

  <entry class="blue">
    <img class="header" src="assets/img/decim7.jpg" />
    <h2>Mesh Decimation by Triangle Collapse</h2>
    <p>As our Vertex-Elimination Decimation had too many caveats to be used as a general purpose decimation algorithm - and even ran into problems with our target of terrain decimation - we'll have to come up with a different solution. So, let's try to address the largest problems we ran into: Dependency on &quot;nice&quot; geometry, that we'd end up ruining eventually anyways & needing to do extra steps to fix vertex-ordering in triangles, if requested.</p>
  </entry>

  <entry class="blue">
    <img class="header" src="assets/img/decim2.jpg" />
    <h2>Mesh Decimation by Vertex Elimination</h2>
    <p>Since Border-Alternating Terrain LODs require full control over <b>which</b> triangles are decimated, a custom mesh decimation algorithm is needed - and even if not, it'd be very convenient to have a fast, reliable &amp; flexible way of decimation at our finger tips.</p>
    <p>So decimating a mesh by eliminating the least-relevant vertices seemed like a novel & fast solution to the problem. But first, how do we want to eliminate those least-relevant vertices? <gray>[Cover Image: <a href="https://sketchfab.com/3d-models/grabfigur-3d50bc5f798f44d39e8030d3b752360e">CC BY-NC 4.0</a>]</gray></p>
    <img class="small" src="assets/img/decim3.png">
    <br>
    <p>Let's assume, we have this geometry-fragment, and we've determined, that the tip of this shape is the vertex we'd like to eliminate. In order to remove said vertex from our object, we have to ensure, that all neighboring vertices that previously connected to the tip do now connect to one another. So, by observing the vertices in the triangles that touch our chosen vertex, we can construct the polygon(s) of resulting area. This polygon can now simply be triangulated and our vertex can be removed entirely.</p>
    <img class="small" src="assets/img/decim4.png">
    <br>
    <p>As simple as that may seem, this is much harder to implement in practice, that it may seem. Polygon triangulation in 3D isn't really a thing if the vertices don't all lie on a flat plane and even 2D polygon triangulation isn't trivial at all. As we're merely evaulating the suitability of this approach, we'll simply ignore those cases for now, and disallow any vertices from being eliminated if this would result in such a horrible case.</p>
    <p>Regarding our goal of being able to leave singular vertices untouched, to line up with higher-fidelity LODs, this can be achieved very easily here, as the decimation procedure happens <b>per-vertex</b> anyways.</p>
    <br>
    <h3>Error Calculation</h3>
    <p>In order to prioritize vertices that result in minimal change to the mesh, we need to define an error function. Fortunately, this is rather easy: The error of removing a the vertex at the tip can be formulated as the area between the re-triangulated base and the tip. As there's no singular valid triangulation of a polygon, this calculation unfortunately has to be repeated for all vertices and can't be blindly carried over to neighbors, as they may end up with a different triangulation, that would result in a different error value. To demonstrate, that not only <b>one</b> valid triangulation exists, let's consider the case of the following opening and its <b>two different</b> triangulations:</p>
    <img class="small" src="assets/img/decim5.png">
    <br>
    <h3>Performance</h3>
    <p>This approach is actually blazingly fast, even when forced to re-triangulate the resulting areas from the perspective of each vertex. To decimate the Statue pictured at the top of this entry to the point where all vertex eliminations would result in horrific polygons, this is what we end up with:</p>
    <code>
      Error Calculation (incl. triangulation of base): 632,974 tris in 0.06161 s (3,449,320 non-skipped /s).<br>
      Error Sorting: 0.02669 s (11,859,484 /s).<br>
      Elimination & Re-Triangulation for 150,081 verts: 0.60876 s (246,536.4 /s).<br><br>
      Decimated 316,481 verts, 632,974 tris to 166,400 verts, 332,812 tris. (removed 15,0081 verts, 300,162 tris)
    </code>
    <p>This required an incredibly fast chunked list data-structure to be written, which allows for blazingly fast random removals & inserts (removing old error values in a sorted list, inserting new error values at the correct position) and a bunch of other little tricks.</p>
    <br>
    <h3>Complications</h3>
    <p>What should now be somewhat apparent from the Performance Output above is, that many of the vertices needed to be skipped, as it would've been too complex to triangulate the resulting geometry. This usually isn't as terrible with smaller models, but heavily impacts the flexibility of this algorithm. What can be somewhat worked around, but isn't particularly pleasing about this approach is, that triangle vertex-ordering (which is sometimes used to determine normal directions and allows for front- or backface culling) isn't automatically preserved and needs to be <i>fixed</i> during the decimation process, if this behaviour is relevant.</p>
    <img src="assets/img/decim6.jpg">
  </entry>

  <entry class="blue">
    <img class="header" src="assets/img/decim0.png" />
    <h2>Border-Alternating Terrain LODs</h2>
    <p>With connected geometry - like terrain geometry - simply disecting the model into tiles and providing various LODs for each tile (with larger tile sizes for lower quality LODs) unfortunately leads to problems whenever different LOD layers meet, as with a naive approach LOD levels would disagree at their borders about the level of fidelity. Forcing higher quality rims onto lower quality levels leads to an escalation, where even the lowest quality LODs would need to provide the highest quality borders, even though they would never actually border any of the highest quality levels, but as every lower level needs to be able to connect to a higher and lower quality level seamlessly, this forces highest quality borders onto <b>all</b> layers, which will increase the geometric detail of the lowest-quality LODs so much, that it entirely defeats the purpose of having LOD levels in the first place, as all of the detail is spent on irrelevant high-fidelity borders.</p>
    <img class="small" src="assets/img/decim1.png">
    <br>
    <p>In order to resolve this, LOD levels can't share borders across <b>all</b> layers. This makes streaming LODs and generating LODs a lot more complex, but completely eliminates the issue at hand: Let's assume we have this lowest-quality <mono>LOD0</mono> of our terrain.</p>
    <img class="small" src="assets/img/lod0.png">
    <br>
    <p>If we want to load a segment of <mono>LOD0</mono>, we need to split it into its four children, as otherwise we'd end up rendering some segments twice: Once with <mono>LOD0</mono> and more with <mono>LOD1</mono>.</p>
    <img class="small" src="assets/img/lod1.png">
    <br>
    <p>Now, to address the issue discussed above, we need to choose a different boundary, at which the lower level lods line up, so let's lock vertices at the dark-turquoise lines to match the fidelity of <mono>LOD2</mono>.</p>
    <img class="small" src="assets/img/lod2.png">
    <br>
    <p>Let's assume the worst case: We're interested in more detail in the selected tile of <mono>LOD2</mono>. Simply loading in this tile, we would disagree with the neighboring tiles of <mono>LOD1</mono> on the level-of-fidelity at the two borders highlighted in white.</p>
    <img class="small" src="assets/img/lod3.png">
    <br>
    <p>To address this, loading the tile highlighted in white <b>forces</b> the turquoise tiles to be loaded at the same LOD, as they share the same dark-turquoise square.</p>
    <img class="small" src="assets/img/lod4.png">
    <br>
    <p>In order to again address the issue of sharing boundaries between LOD levels, we force the vertices on the dark-green lines to match the vertices at <mono>LOD3</mono>.</p>
    <img class="small" src="assets/img/lod5.png">
    <br>
    <p>Let's again assume one of the more interesting cases, and let's say we want to load a higher quality LOD at the tile highlighted in green. Similar as before, this leads to a disagreement on the level of fidelity on the borders highlighed in white.</p>
    <img class="small" src="assets/img/lod6.png">
    <br>
    <p>As discussed before, loading in the tile highlighted in white <b>forces</b> the tiles in green to be loades as well. Actually <b>any</b> of these 4 tiles being loaded would <b>force</b> the remaining 3 tiles to be loaded in as well.</p>
    <img class="small" src="assets/img/lod7.png">
    <br>
    <p>As two of the parent nodes of those nodes haven't been loaded in already, this also forces <b>those parent nodes</b> to be loaded in at a higher LOD as well, namely <mono>LOD2</mono>.</p>
    <img class="small" src="assets/img/lod8.png">
    <br>
    <p>Implementing this intra-tile dependency isn't actually as tricky as it may seem, as it's always very straightforward to determine, which of the neighboring tiles need to be loaded in as well. A simple look-up-table suffices to store the vectors to the requires neighboring tiles being a certain child-index of a parent tile:</p>
    <code>
      static const vec2i8 siblings_from_child_idx[4][3]{<br>
      &emsp;&emsp;{{-1, -1}, {0, -1}, {-1, 0}}, // top left<br>
      &emsp;&emsp;{{0, -1}, {1, -1}, {1, 0}}, // top right<br>
      &emsp;&emsp;{{-1, 0}, {-1, 1}, {0, 1}}, // bottom left<br>
      &emsp;&emsp;{{1, 0}, {0, 1}, {1, 1}} // bottom right<br>
      };
    </code>
    <p>Now one can simply recursively iterate over tiles by starting at the root, splitting every tile that is sufficiently close and loading in all sibling tiles that need to be loaded alongside a given child.</p>
  </entry>

  <entry class="green">
    <img class="header" src="assets/img/gi0.jpg" />
    <h2>Single Bounce Voxel Traced Global Illumination</h2>
    <p>Now, that we've voxelized our shaded scene, the main deferred pass can be followed up by an additively blended global illumination pass. In this pass we use the same g-buffers as inputs as in the regular deferred shading pass, but instead of looking up lights and shading based on surface color and normal, we take the world-position as a starting point, reflect the camera-to-world-space-vector at the surface normal, move slightly (with a small random offset) into the derived direction (until we're no longer in the vicinity of neighboring voxels), add some random rotation to the direction vector (based on material roughness) and trace through the voxelized geometry until we either run out of steps (in this case, we assume, that we've hit the skybox and sample from it), or we've hit a shaded or unshaded populated voxel, in which case we derive the color from said voxel.</p>
    <p>Even just sampling with one ray per pixel results in reasonably smooth results when viewed at sufficiently high framerates. This will eventually be smoothed out by TAA, so screenshots won't end up looking horribly noisy in the final version.</p>
    <img src="assets/img/gi1.jpg">
    <br>
    <h2>Effective Voxel Tracing</h2>
    <p>Given a voxel position <mono>pos</mono> (i.e. <mono>[1, 4, 7]</mono> in the voxel field <mono>[0, 0, 0] ~ [10, 10, 10]</mono>) and direction <b>dir</b>, with the sampled voxel being <mono>cpos = pos + t * dir</mono>, we can immediately transition to the next voxel by using the following basic formula:</p>
    <code>
      vec3 delta = vec3(1) - fract(cpos);<br>
      vec3 delta_scaled = delta / dir;<br>
      t += min(delta_scaled.x, min(delta_scaled.y, delta_scaled.z));
    </code>
    <br/>
    <h2>Generating Randomly Rotated Vectors For Rough Surfaces</h2>
    <p>Randomly rotating arbitrary vectors can be somewhat cumbersome, we use the following approach:</p>
    <ol>
      <li>Take the unit up-vector (<mono>[0, 0, -1]</mono> in our case)</li>
      <li>Randomly rotate it in x and y direction (or whatever two directions aren't up in your preferred coordinate system)</li>
      <li>Generate a mapping matrix, that rotates the unit up-vector onto our original vector using the previously discussed <b>RotateAlignVectors</b> function.</li>
      <li>Transform the rotated up-vector with the mapping matrix</li>
    </ol>
    <p>As this doesn't actually rotate arbitrary vectors but merely aligns the rotated vector with the up-vector, this approach doesn't suffer from the same issues as more straightforward implementations.</p>
  </entry>

  <entry class="green">
    <img class="header" src="assets/img/voxel0.png" />
    <h2>Hardware Voxelization</h2>
    <p>To be able to cheaply trace rays through the rendered scene, all static geometry is hardware voxelized through a clever combination of multisample anti-aliasing (MSAA) and a geometry shader that maximizes rasterized surface area.</p>
    <p>As the color-resolution of the voxelized scene doesn't need to be particularly high or temporally stable, voxel colors are dithered and logarithmically encoded with only 8-bits per voxel (3 bits red, 3 bits green, 2 bits blue with 0 = no voxel).</p>
    <p>Performant shading is provided through a world-space-clustering of the voxelized area, which works very similarly to the view-space clustering discussed previously, but instead of splitting the view-frustum into smaller pieces, the entire world-space in voxelizer range is split into regions, approximated by bounding spheres (if the voxelized area doesn't rotate, the bounding sphere calculation only needs to happen <b>once</b>, as all subsequent transforms are merely translations of the pre-existing clusters), then the bounding spheres are intersected with the light frustums in a cpu-side lighting pre-pass and stored in a separate lookup table.</p>
    <img src="assets/img/voxel1.png">
  </entry>

  <entry class="blue">
    <img class="header" src="assets/img/cluster0.png" />
    <h2>Clustered Deferred Lighting</h2>
    <p>To significantly reduce the number of required shadow map lookups in a fragment shader, the view frustum can be subdivided into many small frustums in a pre-process (which we approximate by their bounding sphere). We can now check for the intersection of the light-view-frustum and view-frustum-segment-bounding-sphere and reference the relevant light indices of a given segment in a small lookup table.</p>
    <p>Retrieving the corresponding cluster index in the fragment shader is very straightforward, as the only information required to resolve the fragment to a cluster is the screenspace texture coordinate and linearized depth ratio between the near and far plane. All of these inputs are already commonplace in deferred shading setups.</p>
    <p>Additionally, such a lookup can easily be utilized to implement a basic volumetric lighting shader.</p>
    <img src="assets/img/cluster1.png">
  </entry>

  <entry class="blue">
    <video class="header" loop autoplay="true" muted="true">
      <source src="assets/vid/blended_terrain.mp4" type="video/mp4">
    </video>
    <h2>Procedurally generated PBR textures</h2>
    <p>To fill our procedurally generated worlds with unique looking environments, we render the corresponding colors for a given region directly into the texture atlas.</p>
    <p>As material indexes are directly passed to the shader, alongside the world-position of the corresponding terrain, we can select the correct type of texture and procedurally generate the values to the atlas. This results in a lot of variety, even if the same material is used:</p>
    <br/>
    <table>
      <tr><th>Snowy Rocks</th><th>Ice</th></tr>
      <tr>
        <td>
          <video class="table" loop autoplay="true" muted="true">
            <source src="assets/vid/snowy_rocks.mp4" type="video/mp4">
          </video>
        </td>
        <td>
          <video class="table" loop autoplay="true" muted="true">
            <source src="assets/vid/ice.mp4" type="video/mp4">
          </video>
        </td>
      </tr>
    </table>
    <p>Blending is done on by simply applying a noise function to the barycentric coordinates that are used to fetch the per-vertex material index in the fragment shader from a texture. Sadly it's currently not possible to use different noise patterns for different kinds of material combinations, but the current noise pattern results in very pleasing material transitions already (see video in the header)</p>
    <p>This way we can create a wide variety of procedural textures and selectively apply them to the underlying terrain. These are all of the material types we currently offer:</p>
    <video loop autoplay="true" muted="true">
      <source src="assets/vid/materials.mp4" type="video/mp4">
    </video>
  </entry>

  <entry class="blue">
    <img class="header" src="assets/img/atlas2.png" />
    <h2>Improving and Optimizing the Texture Atlas</h2>
    <p>The original implementation was already capable of processing hundreds of thousands of triangles per second, but the utilization of the texture area wasn't particularly good, as the coverage quadtree initially preferred placing triangle clusters at high-layer node positions. This left arbitrarily sized holes in the quadtree that are neither particularly easy to find, when looking for available space for future clusters nor necessarily good spots to place clusters into, as the total available area is quickly fragmented by the first couple of clusters.</p>
    <p>We'd much prefer clusters to be placed closer to existing clusters, so we'll modify the atlas slightly: After the final dimensions have been found, we allow the quadtree to attempt to move the current bounds to the top left by one node per layer if possible. This achieves much better atlas coverage - still not optimal, but improved to the point that models with double the unit-size can consistently fit into the atlas now. Apart from the coverage benefits, for some models this ends up positively impacting the runtime performance as well, as it's much faster to find empty spots with the improved coverage.</p>
    <p>After a bunch of smaller optimization (mostly bloom-filters to avoid costlier randomish-memory-access-dependent-conditionals) we're somewhere between handling <b>500k - 1M Triangles per Second</b> on a single thread.</p>
    <h3>Stanford Dragon Texture Coordinates & Model Render</h3>
    <img src="assets/img/atlas3.png">
    <p>To actually get a decent performance benchmark of the texture atlas, I fed it the entire stanford dragon model (<b>871k Triangles</b>), as well as a bunch of other large standard models (like a simplified version of Stanford Lucy).</p>
    <code>
      2.01443 s (432,422 triangles per second) without moving cluster bounds to the top-left<br>
      0.87905 s (990,936 triangles per second) with moving cluster bounds to the top-left
    </code>
    <p>Although, with the simplified Stanford Lucy model however (28k Triangles), the old method appears still to be faster - probably because of the larger variance in terms of triangle area:</p>
    <code>
      0.02538 s (1,105,358 triangles per second)  without moving cluster bounds to the top-left<br>
      0.05534 s (506,846.8 triangles per second)  with moving cluster bounds to the top-left
    </code>
  </entry>
  
  <entry class="blue">
    <img class="header" src="assets/img/atlas1.png" />
    <h2>Fast Texture Atlas for Procedural Geometry</h2>
    <p>Since world chunks are going to be represented with consistent, but procedural geometry, we need an efficient way of generating texture information for chunks to make manual changes and unique-looking locations possible. To sample and store the corresponding texture, we need an efficient way of generating texture coordinates for our procedural geometry.</p>
    <p>As many existing texture atlas implementations aren't particularly fast or assume that the geometry already comes with UV partitioning and texture coordinates that only need to be assembled into one big atlas, I chose to implement a new texture atlas system for the engine.</p>
    <p>The foundation of the texture atlas is a coverage-quadtree where nodes are either empty, partially-covered or full. Nodes are stored in a large buffer as there's no need to stream information on demand. The last layer is merely represented as a large bitmap, rather than the actual nodes containing information about the pixel contents. Utilizing this coverage-quadtree it's fairly straightforward to check if a certain amount of space is currently available in the atlas.</p>
    <p>The atlas procedure consists of two steps:</p>
    <h3>Setup</h3>
    <p>Add all triangles with 3d position and vertex indexes to the atlas. The atlas calculates the side lengths and angles of the triangle and takes note of existing triangles that share an edge with the current one.</p>
    <h3>Texture Coordinate Generation</h3>
    <p>After all triangles have been added, they are first sorted by area. Then chunks of triangles are attempted to be added to the atlas branching out from the initial large triangle's edges.</p>
    <p>There's some special cases that have to be handled here, like when all three vertexes of a triangle are already present in the atlas and connecting them into a new triangle would result in a reasonable approximation of sizes and angles in respect to the original 3d geometry. Likewise, in order to ensure that the resulting areas don't get out of hand with curved geometry, as angles are used to construct the 2d triangles in the texture atlas and the resulting area may slowly creep up or down in size compared to their 3d counterparts, mismatching triangles are rejected from the current batch and will either be added separately or as part of another triangle-cluster.</p>
    <p>Whenever no more connecting triangles would fit into the area available in the texture atlas, triangle bounds are marked as covered and we move on to the next possible collection of triangles - starting with the next largest triangle that hasn't been placed yet.</p>
    <p>Obviously this doesn't result in optimal utililzation of the texture atlas, especially as the bounding rectangle of a triangle is a terrible estimate of it's actual coverage, but this comes with a large performance benefit, as no triangle rasterization is required and the texture generation part can be offloaded to the GPU.</p>
  </entry>
  
  <entry>
    <img class="header" src="assets/img/pixel0.png" />
    <h2>Pixels, Shell Texturing & Grass Height</h2>
    <p>Achieving tripple-A quality graphics isn't entirely feasible with just the limited time of one person, so choosing a distinct, pleasant, and lower-fidelity art-style is very important. To force myself to think more about the broad picture than the low-level details, I halved the display resolution, using my trusty old aliasing-pixelartify algorithm for a neat pixely look.</p>
    <p>Now to the fun part: Moss. Well, shell texturing.</p>
    <p>To supplement the grass and enable moss growth on rocks and other similar surfaces, we create a vegetation-height map that is a half-terrain-resolution approximation of the height of grass - or only moss, if the values are too low to allow grass to grow. We can now color the grass depending on the length to achieve some more visual interest.</p>
    <p>For moss, we create two LODs of instanced heightmap textures with varying height attribute. We create a moss height, normal direction & color change texture that is statically used for all moss and sample the depth from this texture in the fragment shader to conditionally discard (I don't want to, but alpha blending would be a lot worse) fragments.</p>
    <p>Giving the height channel some bumps here and there and sampling a different color depending on the moss height leaves us with a nice fluffy mossy texture that nicely complements the grass.</p>
    <img src="assets/img/pixel1.png">
  </entry>

  <entry>
    <img class="header" src="assets/img/grass0.jpg" />
    <h2>Grass Rendering & Chunk View Culling</h2>
    <p>Flat untextured terrain however isn't particularly pleasing to look at, so - after implementing a chunk view culling system and reducing the draw-calls for terrain rendering to one draw-call per chunk type and refactoring the renderer a bit, let's have a look at grass rendering.</p>
    <p>I'm not the biggest fan of alpha textured grass rendering as the constant overdraw fragment discarding sounds very unpleasant to me. As modern GPUs are capable of rendering a lot of instanced geometry very quickly and having individual segmented blades of grass allows for nice curving grass in the wind, we'll use that approach. Depending on how close the chunk containing the grass is to the camera, we'll use a different LOD on the grass segmentation as even reasonably close chunks can visually represent blades of grass with just a single triangle without a perceptual loss in quality.</p>
    <p>This doesn't cover chunks yet, that are further off in the distance, but depending on how well we can tune the shaders later on, we might be able to either render billboards with large amounts of grass, or even fake the existence of grass entirely with a decent grass-landscape texture.</p>
  </entry>

  <entry class="blue">
    <img class="header" src="assets/img/scape6.jpg" />
    <h2>Terrain Chunk Streaming & Atmospheric Effects</h2>
    <p>In order to traverse such a large scale terrain, LOD & streaming need to be considered immediately. Chunks of different sizes are streamed into fixed-size textures and are dynamically loaded when needed.</p>
    <p>Whenever a large chunks would contain a chunk with vertexes that are closer than a set distance from the camera, the chunk is split and the contained chunks of higher fidelity are streamed in.</p>
    <p>To reduce the pop-in artifacts usually seen in even tripple-A productions, we use local maxima - rather than average terrain height - to construct LODs.</p>
    <p>Obviously this leaves visible gaps in the landscape wherever multiple chunk-types meet, but we'll deal with specific meshes to solve this problem at a later date.</p>
    <h3>A low LOD texture atlas</h3>
    <img src="assets/img/atlas0.png">
    <br>
    <h2>Atmospheric Effects</h2>
    <p>The sun position is now 'rotated' into the coordinate space mapped around the 'flat' planet and camera position using a vector alignment matrix:</p>
    <code>
      inline static matrix RotateAlignVectors(const vec3f fromNormalized, const vec3f ontoNormalized)
 <br/>{
 <br/>&ensp;&ensp;vec3f axis = vec3f::Cross(ontoNormalized, fromNormalized);
 <br/>&ensp;&ensp;
 <br/>&ensp;&ensp;const float_t cosA = vec3f::Dot(ontoNormalized, fromNormalized);
 <br/>&ensp;&ensp;const float_t k = 1.0f / (1.0f + cosA);
 <br/>&ensp;&ensp;
 <br/>&ensp;&ensp;matrix result;
 <br/>&ensp;&ensp;
 <br/>&ensp;&ensp;result._11 = (axis.x * axis.x * k) + cosA;
 <br/>&ensp;&ensp;result._12 = (axis.y * axis.x * k) - axis.z;
 <br/>&ensp;&ensp;result._13 = (axis.z * axis.x * k) + axis.y;
 <br/>&ensp;&ensp;result._21 = (axis.x * axis.y * k) + axis.z;
 <br/>&ensp;&ensp;result._22 = (axis.y * axis.y * k) + cosA;
 <br/>&ensp;&ensp;result._23 = (axis.z * axis.y * k) - axis.x;
 <br/>&ensp;&ensp;result._31 = (axis.x * axis.z * k) - axis.y;
 <br/>&ensp;&ensp;result._32 = (axis.y * axis.z * k) + axis.x;
 <br/>&ensp;&ensp;result._33 = (axis.z * axis.z * k) + cosA;
 <br/>&ensp;&ensp;result._44 = 1.f;
 <br/>&ensp;&ensp;
 <br/>&ensp;&ensp;return result;
 <br/>}
    </code>
    <p>
      The way the sun passes through the atmosphere is approximated with an iterative shader that draws an equirectangular view and approximates rayleigh and mie scattering with a fixed number of samples. The scale of the planetary system and atmosphere have been adjusted to achieve earth-like atmospheric conditions.
    </p>
    <img src="assets/img/sky0.png">
  </entry>

  <entry class="blue">
    <img class="header" src="assets/img/scape4.jpg" />
    <h2>Micro-Biome Extraction</h2>
    <p>After the terrain has been generated and eroded, regions need to be classified into biomes based on the physical properties of their surroundings. To calculate these biomes like various grass types wherever the top soil is grass a noise map can be sampled. However this isn't entirely accurate as especially regional & global elevation changes and water proximity can influence biome parameters significantly.</p>
    <p>Therefore we compute a signed distance field of water and one that maps terrain height, blur the height map over a large area and then compute the local difference to an areas neighbors. This leaves us with the water map and the map shown above for local terrain elevation changes.</p>
    <p>Using these two maps and a noise buffer, we can derive micro-biome features like mossy stones near water or in crests, small puddles can be turned to mud, large elevation change inhibit the growth of trees or tall grass, ...</p>
    <img src="assets/img/scape5.jpg">
  </entry>

  <entry class="blue">
    <img class="header" src="assets/img/scape2.jpg" />
    <h2>Wind Erosion</h2>
    <p>With our newly developed surface wind simulation, we can now erode the surface terrain. Terrain that's close to the wind height will be eroded more than terrain that's further from it and wind speeds influence how far particles are dragged when eroded. The various terrain layers have different properties of how easily they can be erroded by wind & water.</p>
    <p>As the eroded particles are dragged through the wind, the terrain is iteratively checked for matching height as the particles slowly descend based on their weight.</p>
    <p>Similarly to the wind simulation, eroded particles are added onto a separate buffer and are applied to the new terrain after all cells have completed their erosion procedure.</p>
    <img src="assets/img/scape3.jpg">
  </entry>

  <entry>
    <img class="header" src="assets/img/wind1.jpg" />
    <h2>Simulating Wind</h2>
    <p>In order to arrive at realistic wind conditions to drive wind erosion, both atmospheric and surface conditions must be simulated. The atmospheric wind simply boils down to</p>
    <ul>
      <li>Generally air is dragged in the opposite direction of the planetary rotation through the coriolis force.</li>
      <li>The air in each atmospheric cell warms up based on solar energy reaching the surface (and slowly cools to the temperature of outer space when there's no incoming solar energy)</li>
      <li>Based on the temperature and the air density at any position, calculate the pressure</li>
      <li>Move air-mass (density) velocity around to attempt to equally distribute air pressure</li>
    </ul>
    <p>The last step can be applied iteratively to support fine-grained air movements. Whilst iterating the exiting wind buffer, resulting wind is accumulated in a separate buffer to be applied after all cells have been processed.</p>
    <p>One trick that specifically helped simulating sub-cell movements over multiple iterations - by slightly breaking the laws of physics - was to multiply the wind speeds in cells that don't have enough wind speed to make any pressure move from one cell to the next. This eventually tips the scales and makes the wind in those low pressure cells travel small distances over large amounts of time consistently.</p>
    <h3>Atmospheric Air Density and Wind Speed Vectors</h3>
    <img src="assets/img/wind0.jpg">
    <br>
    <h2>Surface Wind</h2>
    <p>Surface wind derives its temperature from the reflectivity and temperature emission of the underlying material. Surface Wind can be imaginged as a sheet of cloth that's being dragged over the terrain surface. Wind Speeds slow down (in my implementation even reverse slightly) when moving against higher terrain. This may result in the wind height being raised at a given position, or the wind preferring another direction over the current brute-force approach.</p>
    <p>Falling winds to the contrary even enjoy a slight speedup to their wind speeds. This results in pressure zones being separated by mountain ranges whilst wind bursts through the cracks in order to equalise the pressure on both sides.</p>
    <p>Surface winds are slightly dragged along with the general atmospheric wind.</p>
    <h3>Surface Air Density and Wind Speed Vectors</h3>
    <img src="assets/img/wind2.jpg">
  </entry>

  <entry class="blue">
    <h2>Orbital Dynamics</h2>
    <p>In order to have semi-accurate climatic conditions producing terrain with only one hot and one cold zone, an somewhat excentric elliptic orbital path with fine tuned rotational velocity and direction is required. Solving orbital dynamics formulas - even with a fixed star that isn't affected by the satellites mass - proved quite difficult however.</p>
    <p>In Polar Coordinates, the orbital distance from the origin at angle <mono>theta</mono> with orbital parameters <mono>a</mono> and <mono>b</mono> can be expressed like this:</p>
    <code>
     radius(theta) := (a * (1 - b * b)) / (1 + b * cos(theta))
    </code>
    <p>This alone doesn't help much however, as the velocity at each position can easily be calculated, but woulnd't  translate to a closed form solution and I didn't want to resort to step-wise simulation that'd be hard to rely on or accurately predict with different time steps. So, I was able to construct a function <mono>angle(time)</mono> to derive the radius by pluggin the result into the <mono>radius</mono> function:</p>
    <code>
      angle(time) := 2 * atan(sqrt(-b / (b - 1) - 1 / (b - 1)) * tan((pi * combinedMass * time) / (2 * a) - pi * pi / (2 * a)))
    </code>
    <p>In order to arrive at a combined mass that actually produces a period of <mono>2 * pi</mono>, we have to calculate the mass from orbital parameter <mono>a</mono>:</p>
    <code>
      combinedMass(a) := (2 * a * pi / 2) / (pi * pi)
    </code>
    <p>As the combined mass of the two orbital bodies doesn't change with the system, it can be calculated on initialization when setting the other orbital parameters.</p>
    <h3>Planets, Average Solar Energy on the Surface, Current Solar Energy</h3>
    <img src="assets/img/planet0.png"/>
  </entry>

  <entry>
    <img class="header" src="assets/img/scape0.jpg" />
    <h2>Hydrolic Erosion</h2>
    <p>The layered terrain can be eroded using simulated water-droplets that carry material based amounts of sediment around. Finding the path to the deepest reachable point (or until all water has evaporated) for each block of terrain is vastly faster than simulating only one step at a time, but produces much rougher terrain than stepwise erosion. Stepwise erosion smoothes out the terrain and forms large water streams that then decend down to the valleys, where the small crests created from the first few water droplets in the decent-per-droplet approach won't widen and will preferrably be used by later droplets that run down similar paths, not resulting in broadening of common water paths.</p>
    <h3>Layered Terrain Before / After Water Erosion</h3>
    <img src="assets/img/scape1.jpg" />
  </entry>

  <entry class="blue">
    <img class="header" src="assets/img/layer0.jpg" />
    <h2>Layer Based Terrain</h2>
    <p>Terrain is built in layers, the bottom layer is non-erodable bedrock. Depending on a signed-distance field, a noise layer and two cubic slopes a certain height of terrain of a given layer will be added on top of the existing terrain. The signed distance field chooses the min and max value from the two cubic slopes (basically a min- and max height) and the noise value (between 0 and 1) is used to smoothly interpolate between those two values. The generated terrain is rough at first, but erosion will massively improve and shape those layers later.</p>
    <code>
      const float_t min = a_min * x * x * x + b_min * x * x + c_min * x + d_min;<br/>
      const float_t max = a_max * x * x * x + b_max * x * x + c_max * x + d_max;<br/>
      const float_t height = lerp(min, max, noiseValue);<br/>
      <br/>
      if (height > 0)<br/>
      {<br/>
        &ensp;&ensp;// deposit layer onto terrain.<br/>
      }<br/>
    </code>
  </entry>

  <script>
    var active_target = null;
    var color_lut = { green: 'rgb(86, 105, 101)', red: 'rgb(71, 62, 51)', blue: 'rgb(70, 94, 107)' };

    function scroll_callback() {
      var target = null;
      var elements = document.getElementsByTagName('entry');

      for (var x of elements) {
        if (x.getBoundingClientRect().y < screen.height / 2)
          target = x;
      }

      if (active_target != target) {
        var class_name = target.className;
        
        if (class_name == '')
          class_name = 'green';

        document.body.style.backgroundColor = color_lut[class_name];
      }
    }

    document.addEventListener('scroll', scroll_callback);
  </script>
</body>

</html>